{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beb34f8e-e013-48ee-9ec4-3519ba051c2f",
   "metadata": {},
   "source": [
    "**Load Tokenized Dataset or Download the base dataset (not tokenized) from hf**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b336235-c97b-4b7e-80e5-16d66d475ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "from pathlib import Path\n",
    "\n",
    "#Initialize a bool to check if the tokenized dataset exists in disk or if it has to be tokenized \n",
    "tokenized_dataset_exists = False\n",
    "\n",
    "dataset_path = Path(\"./data/tokenized_dataset\")\n",
    "\n",
    "if dataset_path.exists():\n",
    "    tokenized_dataset = load_from_disk(str(dataset_path))\n",
    "    tokenized_dataset_exists = True\n",
    "    print(\"Dataset loaded successfully!\")\n",
    "else:\n",
    "    print(\"Dataset does not exist at the specified path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b858135-492c-4432-b8c9-75cd5352c538",
   "metadata": {},
   "source": [
    "**If the tokenized dataset doesnt exist on disk we download it and tokenize it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39981374-6d95-45a1-9cf2-7c2a9331f58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping the download of whole dataset from Huggingface\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "if tokenized_dataset_exists == False:\n",
    "    # Load the dataset and specify the cache directory\n",
    "    whole_dataset = load_dataset(\"wikimedia/wikipedia\", \"20231101.es\", cache_dir=\"./data\")\n",
    "    ds = ds[\"train\"].train_test_split(test_size=0.98, seed=42) #We split the dataset to make it smaller and \n",
    "    #try different training parameters faster\n",
    "else:\n",
    "    print(\"Skipping the download of whole dataset from Huggingface\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e9a7a9-3660-416f-8f70-eee2b321f5a9",
   "metadata": {},
   "source": [
    "## We load the tokenizer from disk or create the tokenizer if it doesnt exist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cb96ad-db75-4c61-a223-9269027d7c8e",
   "metadata": {},
   "source": [
    "Function to load the text from the dataset to tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a69b9a2-6cb9-4456-bd38-03e32befdaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_corpus():\n",
    "    for sample in ds[\"train\"]:\n",
    "        yield sample[\"text\"]  # Extract text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c31694-6d8c-4fbc-9ebd-128c0d1cb93c",
   "metadata": {},
   "source": [
    "**Load tokenizer // Create tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f582584-3b7a-4bfb-aacb-b9f921e9f5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded! üéâ\n",
      "[42, 8735, 21776, 3]\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from datasets import load_from_disk\n",
    "from pathlib import Path\n",
    "\n",
    "#Initialize a bool to check if the tokenized dataset exists in disk or if it has to be tokenized \n",
    "tokenized_dataset_exists = False\n",
    "\n",
    "tokenizer_path = Path(\"./data/scratch_tokenizer.json\")\n",
    "\n",
    "if tokenizer_path.exists():\n",
    "    #load tokenizer if its saved on disk\n",
    "    hf_tokenizer = PreTrainedTokenizerFast(tokenizer_file=str(tokenizer_path), \n",
    "                                       unk_token=\"[UNK]\", \n",
    "                                       pad_token=\"[PAD]\", \n",
    "                                       mask_token=\"[MASK]\")\n",
    "    # Save it in the Hugging Face format\n",
    "    hf_tokenizer.save_pretrained(\"custom_tokenizer\")\n",
    "    print(\"Tokenizer loaded! üéâ\")\n",
    "else:\n",
    "    tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "    trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[MASK]\"], vocab_size=30_000)\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)\n",
    "    tokenizer.save(\"./data/scratch_tokenizer.json\")\n",
    "    print(\"Tokenizer training complete! üéâ\")\n",
    "\n",
    "\n",
    "# Test with Transformers API\n",
    "print(hf_tokenizer.encode(\"Hello world!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5879d5-413e-455a-b561-b8787f731641",
   "metadata": {},
   "source": [
    "**Tokenize the dataset if not in disk**\n",
    "\n",
    "At the beginning we downloaded the whole dataset in the case the tokenized one didnt exist\n",
    "but we havent tokenized it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13d39b0f-15ec-44d3-bfba-d7d2be6ae4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize text in 128-token chunks\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize text and split into chunks of max length 128\n",
    "    tokenized_text = hf_tokenizer(\n",
    "        examples[\"text\"], \n",
    "        truncation=True, \n",
    "        padding=\"max_length\", \n",
    "        max_length=128\n",
    "    )\n",
    "    return tokenized_text\n",
    "    \n",
    "if not dataset_path.exists():\n",
    "    # Apply tokenization to dataset\n",
    "    tokenized_dataset = ds[\"train\"].map(tokenize_function, batched=True)\n",
    "    tokenized_dataset.save_to_disk(str(dataset_path))\n",
    "    print(f\"Dataset tokenized and saved to disk on path {string(dataset_path)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad72fbf2-287d-44f5-bc6f-d94457520093",
   "metadata": {},
   "source": [
    "## Creating a data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7c95a2f-2f97-4423-9ea5-b07698665dc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([36823, 128])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Convert dataset to PyTorch format\n",
    "tokenized_text = torch.tensor(tokenized_dataset[\"input_ids\"])\n",
    "tokenized_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7aba66b-803f-43b9-9881-01382baa2651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Data Shape: torch.Size([36823, 127])\n",
      "Target Data Shape: torch.Size([36823, 127])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Shift left to create targets\n",
    "input_data = tokenized_text[:, :-1]  # Remove LAST token in each sequence\n",
    "target_data = tokenized_text[:, 1:]  # Remove FIRST token in each sequence\n",
    "\n",
    "print(\"Input Data Shape:\", input_data.shape)  # Should be (6455, 127)\n",
    "print(\"Target Data Shape:\", target_data.shape)  # Should be (6455, 127)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ab4897d-c94b-46c8-8872-c1239b24d0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "batch_size = 64\n",
    "dataset = TensorDataset(input_data, target_data)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True,  num_workers=4, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bae10942-c80d-47cd-875e-a400e3c166d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Both pytorch and HF tokenizer will want to use many cpu cores, and will result in error\n",
    "## Since we already tokenized the whole corpus we can disable the parallelism of the hf tokenizer\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82b66cd9-d30c-4c9a-8dc0-f6e0dd6704e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch input shape: torch.Size([64, 127])\n",
      "Batch target shape: torch.Size([64, 127])\n"
     ]
    }
   ],
   "source": [
    "# Check batch shapes\n",
    "for batch in data_loader:\n",
    "    inputs, targets = batch\n",
    "    print(\"Batch input shape:\", inputs.shape)\n",
    "    print(\"Batch target shape:\", targets.shape)\n",
    "    break  # St"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e22e1eb-2661-4b8a-ac23-50d2ecca23fe",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb46cec0-6947-4dab-8211-c9ea38135092",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        \n",
    "        # üîπ First layer: Embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # üîÅ LSTM Layers (or use nn.RNN / nn.GRU)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        \n",
    "        # ‚ö° Output layer (for classification, etc.)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, sequence_length]\n",
    "        \n",
    "        embedded = self.embedding(x)  # üé≠ Convert indices to embeddings\n",
    "        output, hidden = self.rnn(embedded)  # üîÅ Recurrent processing\n",
    "        logits = self.fc(output)  # ‚ö° Final classification\n",
    "        \n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f8027f-5942-4d5f-9c79-09c7796ced83",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6635eb69-ee81-456b-a4c6-3ace2c241f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim \n",
    "import torch.amp as amp  # For Automatic Mixed Precision (AMP)\n",
    "\n",
    "vocab_size = 30000   # Number of words in vocabulary\n",
    "embedding_dim = 200  # from 100 to 200, for instance\n",
    "hidden_dim = 256     # from 128 to 256  # Size of RNN hidden state\n",
    "output_dim = vocab_size      # 2 for Binary classification (Positive/Negative) or vocab_size for next token generation\n",
    "num_layers = 8\n",
    "\n",
    "model = SimpleRNN(vocab_size, embedding_dim, hidden_dim, num_layers, output_dim)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr =0.003\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "num_epochs = 10\n",
    "scaler = amp.GradScaler()  # GradScaler for AMP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8662a886-ccb1-4bee-b3c1-e01d078c4619",
   "metadata": {},
   "source": [
    "## Use dataloader batches for smaller inputs for memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "75c61af6-53b2-4448-b594-aaf55b5adb76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68f1651-5312-48d2-8658-9dd42895cc0b",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7ded7cce-9ee3-4062-aec0-654b8f3c8807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Summary:\n",
      "  Total Epoch Time     : 52.84 s\n",
      "  Total Data Loading   : 0.11 s\n",
      "  Total GPU Compute    : 48.48 s\n",
      "  Average Loss         : 4.2657\n",
      "\n",
      "Epoch 2 Summary:\n",
      "  Total Epoch Time     : 52.83 s\n",
      "  Total Data Loading   : 0.11 s\n",
      "  Total GPU Compute    : 48.48 s\n",
      "  Average Loss         : 4.2098\n",
      "\n",
      "Epoch 3 Summary:\n",
      "  Total Epoch Time     : 52.97 s\n",
      "  Total Data Loading   : 0.11 s\n",
      "  Total GPU Compute    : 48.62 s\n",
      "  Average Loss         : 4.1595\n",
      "\n",
      "Epoch 4 Summary:\n",
      "  Total Epoch Time     : 52.88 s\n",
      "  Total Data Loading   : 0.11 s\n",
      "  Total GPU Compute    : 48.54 s\n",
      "  Average Loss         : 4.1119\n",
      "\n",
      "Epoch 5 Summary:\n",
      "  Total Epoch Time     : 53.08 s\n",
      "  Total Data Loading   : 0.11 s\n",
      "  Total GPU Compute    : 48.71 s\n",
      "  Average Loss         : 4.0734\n",
      "\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "model.to(device)\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start = time.time()\n",
    "    total_data_loading_time = 0.0\n",
    "    total_gpu_compute_time = 0.0\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # Initialize batch_start to measure data loading for the first batch.\n",
    "    batch_start = time.time()\n",
    "\n",
    "    for batch in data_loader:\n",
    "        # Once batch is fetched, measure data loading time:\n",
    "        batch_loaded_time = time.time()\n",
    "        total_data_loading_time += (batch_loaded_time - batch_start)\n",
    "        \n",
    "        # Unpack and send to device:\n",
    "        batch_inputs, batch_targets = batch\n",
    "        batch_inputs = batch_inputs.to(device).long()\n",
    "        batch_targets = batch_targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Start GPU compute timing:\n",
    "        gpu_start = time.time()\n",
    "        with amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "            outputs = model(batch_inputs)\n",
    "            loss = criterion(outputs.view(-1, vocab_size), batch_targets.view(-1))\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        gpu_end = time.time()\n",
    "        total_gpu_compute_time += (gpu_end - gpu_start)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Prepare for next iteration: record time after optimizer step.\n",
    "        batch_start = time.time()\n",
    "\n",
    "    epoch_end = time.time()\n",
    "    epoch_duration = epoch_end - epoch_start\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Summary:\")\n",
    "    print(f\"  Total Epoch Time     : {epoch_duration:.2f} s\")\n",
    "    print(f\"  Total Data Loading   : {total_data_loading_time:.2f} s\")\n",
    "    print(f\"  Total GPU Compute    : {total_gpu_compute_time:.2f} s\")\n",
    "    print(f\"  Average Loss         : {avg_loss:.4f}\\n\")\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b03323b6-440d-497a-8e2a-ef19e6527d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def generate_text(model, tokenizer, prompt, num_words=50, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Generate text using the  model.\n",
    "\n",
    "    Args:\n",
    "    - model: \n",
    "    - tokenizer: Tokenizer with `encode()` and `decode()` methods.\n",
    "    - prompt: Seed text to start generation.\n",
    "    - num_words: Number of words to generate.\n",
    "    - device: \"cuda\" or \"cpu\".\n",
    "\n",
    "    Returns:\n",
    "    - Generated text (string).\n",
    "    \"\"\"\n",
    "    model.eval()  # ‚úÖ Set model to evaluation mode\n",
    "    model.to(device)\n",
    "\n",
    "    # üëá Tokenize the input prompt\n",
    "    input_ids = torch.tensor(tokenizer.encode(prompt), dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    for _ in range(num_words):\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids)  # üîç Forward pass\n",
    "            next_token_logits = logits[:, -1, :]  # Take last token's output\n",
    "\n",
    "            # üé≤ Sample the next word (greedy or probabilistic)\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(0)  # Greedy decoding\n",
    "\n",
    "            # Append to input sequence\n",
    "            input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "\n",
    "    # üîÑ Convert token IDs back to text\n",
    "    generated_text = tokenizer.decode(input_ids.squeeze().tolist())\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1000217e-d3aa-4e3b-b9ce-015d6c6c0f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La ciudad de la Rep√∫blica de la Universidad de la Universidad de Buenos Aires fue el √≥rgano de f√∫tbol de la Rep√∫blica de la Rep√∫blica de la Rep√∫blica de la Rep√∫blica de la\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate text\n",
    "prompt_text = \"La ciudad de\"\n",
    "generated_story = generate_text(model, hf_tokenizer, prompt_text, num_words=30)\n",
    "print(generated_story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e5905f-f074-49bd-8d24-fb8cbc7d2dc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
