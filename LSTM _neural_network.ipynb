{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beb34f8e-e013-48ee-9ec4-3519ba051c2f",
   "metadata": {},
   "source": [
    "**Load Tokenized Dataset or Download the base dataset (not tokenized) from hf**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b336235-c97b-4b7e-80e5-16d66d475ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "from pathlib import Path\n",
    "\n",
    "#Initialize a bool to check if the tokenized dataset exists in disk or if it has to be tokenized \n",
    "tokenized_dataset_exists = False\n",
    "\n",
    "dataset_path = Path(\"./data/tokenized_dataset\")\n",
    "\n",
    "if dataset_path.exists():\n",
    "    tokenized_dataset = load_from_disk(str(dataset_path))\n",
    "    tokenized_dataset_exists = True\n",
    "    print(\"Dataset loaded successfully!\")\n",
    "else:\n",
    "    print(\"Dataset does not exist at the specified path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b858135-492c-4432-b8c9-75cd5352c538",
   "metadata": {},
   "source": [
    "**If the tokenized dataset doesnt exist on disk we download it and tokenize it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39981374-6d95-45a1-9cf2-7c2a9331f58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping the download of whole dataset from Huggingface\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "if tokenized_dataset_exists == False:\n",
    "    # Load the dataset and specify the cache directory\n",
    "    whole_dataset = load_dataset(\"wikimedia/wikipedia\", \"20231101.es\", cache_dir=\"./data\")\n",
    "    ds = ds[\"train\"].train_test_split(test_size=0.98, seed=42) #We split the dataset to make it smaller and \n",
    "    #try different training parameters faster\n",
    "else:\n",
    "    print(\"Skipping the download of whole dataset from Huggingface\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e9a7a9-3660-416f-8f70-eee2b321f5a9",
   "metadata": {},
   "source": [
    "## We load the tokenizer from disk or create the tokenizer if it doesnt exist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cb96ad-db75-4c61-a223-9269027d7c8e",
   "metadata": {},
   "source": [
    "Function to load the text from the dataset to tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a69b9a2-6cb9-4456-bd38-03e32befdaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_corpus():\n",
    "    for sample in ds[\"train\"]:\n",
    "        yield sample[\"text\"]  # Extract text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c31694-6d8c-4fbc-9ebd-128c0d1cb93c",
   "metadata": {},
   "source": [
    "**Load tokenizer // Create tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f582584-3b7a-4bfb-aacb-b9f921e9f5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded! 🎉\n",
      "[42, 8735, 21776, 3]\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from datasets import load_from_disk\n",
    "from pathlib import Path\n",
    "\n",
    "#Initialize a bool to check if the tokenized dataset exists in disk or if it has to be tokenized \n",
    "tokenized_dataset_exists = False\n",
    "\n",
    "tokenizer_path = Path(\"./data/scratch_tokenizer.json\")\n",
    "\n",
    "if tokenizer_path.exists():\n",
    "    #load tokenizer if its saved on disk\n",
    "    hf_tokenizer = PreTrainedTokenizerFast(tokenizer_file=str(tokenizer_path), \n",
    "                                       unk_token=\"[UNK]\", \n",
    "                                       pad_token=\"[PAD]\", \n",
    "                                       mask_token=\"[MASK]\")\n",
    "    # Save it in the Hugging Face format\n",
    "    hf_tokenizer.save_pretrained(\"custom_tokenizer\")\n",
    "    print(\"Tokenizer loaded! 🎉\")\n",
    "else:\n",
    "    tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "    trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[MASK]\"], vocab_size=30_000)\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)\n",
    "    tokenizer.save(\"./data/scratch_tokenizer.json\")\n",
    "    print(\"Tokenizer training complete! 🎉\")\n",
    "\n",
    "\n",
    "# Test with Transformers API\n",
    "print(hf_tokenizer.encode(\"Hello world!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5879d5-413e-455a-b561-b8787f731641",
   "metadata": {},
   "source": [
    "**Tokenize the dataset if not in disk**\n",
    "\n",
    "At the beginning we downloaded the whole dataset in the case the tokenized one didnt exist\n",
    "but we havent tokenized it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13d39b0f-15ec-44d3-bfba-d7d2be6ae4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize text in 128-token chunks\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize text and split into chunks of max length 128\n",
    "    tokenized_text = hf_tokenizer(\n",
    "        examples[\"text\"], \n",
    "        truncation=True, \n",
    "        padding=\"max_length\", \n",
    "        max_length=128\n",
    "    )\n",
    "    return tokenized_text\n",
    "    \n",
    "if not dataset_path.exists():\n",
    "    # Apply tokenization to dataset\n",
    "    tokenized_dataset = ds[\"train\"].map(tokenize_function, batched=True)\n",
    "    tokenized_dataset.save_to_disk(str(dataset_path))\n",
    "    print(f\"Dataset tokenized and saved to disk on path {string(dataset_path)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad72fbf2-287d-44f5-bc6f-d94457520093",
   "metadata": {},
   "source": [
    "## Creating a data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7c95a2f-2f97-4423-9ea5-b07698665dc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([36823, 128])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Convert dataset to PyTorch format\n",
    "tokenized_text = torch.tensor(tokenized_dataset[\"input_ids\"])\n",
    "tokenized_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7aba66b-803f-43b9-9881-01382baa2651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Data Shape: torch.Size([36823, 127])\n",
      "Target Data Shape: torch.Size([36823, 127])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Shift left to create targets\n",
    "input_data = tokenized_text[:, :-1]  # Remove LAST token in each sequence\n",
    "target_data = tokenized_text[:, 1:]  # Remove FIRST token in each sequence\n",
    "\n",
    "print(\"Input Data Shape:\", input_data.shape)  # Should be (6455, 127)\n",
    "print(\"Target Data Shape:\", target_data.shape)  # Should be (6455, 127)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ab4897d-c94b-46c8-8872-c1239b24d0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "batch_size = 64\n",
    "dataset = TensorDataset(input_data, target_data)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True,  num_workers=4, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bae10942-c80d-47cd-875e-a400e3c166d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Both pytorch and HF tokenizer will want to use many cpu cores, and will result in error\n",
    "## Since we already tokenized the whole corpus we can disable the parallelism of the hf tokenizer\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82b66cd9-d30c-4c9a-8dc0-f6e0dd6704e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch input shape: torch.Size([64, 127])\n",
      "Batch target shape: torch.Size([64, 127])\n"
     ]
    }
   ],
   "source": [
    "# Check batch shapes\n",
    "for batch in data_loader:\n",
    "    inputs, targets = batch\n",
    "    print(\"Batch input shape:\", inputs.shape)\n",
    "    print(\"Batch target shape:\", targets.shape)\n",
    "    break  # St"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e22e1eb-2661-4b8a-ac23-50d2ecca23fe",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb46cec0-6947-4dab-8211-c9ea38135092",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        \n",
    "        # 🔹 First layer: Embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # 🔁 LSTM Layers (or use nn.RNN / nn.GRU)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        \n",
    "        # ⚡ Output layer (for classification, etc.)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, sequence_length]\n",
    "        \n",
    "        embedded = self.embedding(x)  # 🎭 Convert indices to embeddings\n",
    "        output, hidden = self.rnn(embedded)  # 🔁 Recurrent processing\n",
    "        logits = self.fc(output)  # ⚡ Final classification\n",
    "        \n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f8027f-5942-4d5f-9c79-09c7796ced83",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6635eb69-ee81-456b-a4c6-3ace2c241f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim \n",
    "import torch.amp as amp  # For Automatic Mixed Precision (AMP)\n",
    "\n",
    "vocab_size = 30000   # Number of words in vocabulary\n",
    "embedding_dim = 200  # from 100 to 200, for instance\n",
    "hidden_dim = 256     # from 128 to 256  # Size of RNN hidden state\n",
    "output_dim = vocab_size      # 2 for Binary classification (Positive/Negative) or vocab_size for next token generation\n",
    "num_layers = 8\n",
    "\n",
    "model = SimpleRNN(vocab_size, embedding_dim, hidden_dim, num_layers, output_dim)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr =0.003\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "num_epochs = 10\n",
    "scaler = amp.GradScaler()  # GradScaler for AMP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8662a886-ccb1-4bee-b3c1-e01d078c4619",
   "metadata": {},
   "source": [
    "## Use dataloader batches for smaller inputs for memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "75c61af6-53b2-4448-b594-aaf55b5adb76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68f1651-5312-48d2-8658-9dd42895cc0b",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7ded7cce-9ee3-4062-aec0-654b8f3c8807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Summary:\n",
      "  Total Epoch Time     : 52.84 s\n",
      "  Total Data Loading   : 0.11 s\n",
      "  Total GPU Compute    : 48.48 s\n",
      "  Average Loss         : 4.2657\n",
      "\n",
      "Epoch 2 Summary:\n",
      "  Total Epoch Time     : 52.83 s\n",
      "  Total Data Loading   : 0.11 s\n",
      "  Total GPU Compute    : 48.48 s\n",
      "  Average Loss         : 4.2098\n",
      "\n",
      "Epoch 3 Summary:\n",
      "  Total Epoch Time     : 52.97 s\n",
      "  Total Data Loading   : 0.11 s\n",
      "  Total GPU Compute    : 48.62 s\n",
      "  Average Loss         : 4.1595\n",
      "\n",
      "Epoch 4 Summary:\n",
      "  Total Epoch Time     : 52.88 s\n",
      "  Total Data Loading   : 0.11 s\n",
      "  Total GPU Compute    : 48.54 s\n",
      "  Average Loss         : 4.1119\n",
      "\n",
      "Epoch 5 Summary:\n",
      "  Total Epoch Time     : 53.08 s\n",
      "  Total Data Loading   : 0.11 s\n",
      "  Total GPU Compute    : 48.71 s\n",
      "  Average Loss         : 4.0734\n",
      "\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "model.to(device)\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start = time.time()\n",
    "    total_data_loading_time = 0.0\n",
    "    total_gpu_compute_time = 0.0\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # Initialize batch_start to measure data loading for the first batch.\n",
    "    batch_start = time.time()\n",
    "\n",
    "    for batch in data_loader:\n",
    "        # Once batch is fetched, measure data loading time:\n",
    "        batch_loaded_time = time.time()\n",
    "        total_data_loading_time += (batch_loaded_time - batch_start)\n",
    "        \n",
    "        # Unpack and send to device:\n",
    "        batch_inputs, batch_targets = batch\n",
    "        batch_inputs = batch_inputs.to(device).long()\n",
    "        batch_targets = batch_targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Start GPU compute timing:\n",
    "        gpu_start = time.time()\n",
    "        with amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "            outputs = model(batch_inputs)\n",
    "            loss = criterion(outputs.view(-1, vocab_size), batch_targets.view(-1))\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        gpu_end = time.time()\n",
    "        total_gpu_compute_time += (gpu_end - gpu_start)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Prepare for next iteration: record time after optimizer step.\n",
    "        batch_start = time.time()\n",
    "\n",
    "    epoch_end = time.time()\n",
    "    epoch_duration = epoch_end - epoch_start\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Summary:\")\n",
    "    print(f\"  Total Epoch Time     : {epoch_duration:.2f} s\")\n",
    "    print(f\"  Total Data Loading   : {total_data_loading_time:.2f} s\")\n",
    "    print(f\"  Total GPU Compute    : {total_gpu_compute_time:.2f} s\")\n",
    "    print(f\"  Average Loss         : {avg_loss:.4f}\\n\")\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b03323b6-440d-497a-8e2a-ef19e6527d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def generate_text(model, tokenizer, prompt, num_words=50, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Generate text using the  model.\n",
    "\n",
    "    Args:\n",
    "    - model: \n",
    "    - tokenizer: Tokenizer with `encode()` and `decode()` methods.\n",
    "    - prompt: Seed text to start generation.\n",
    "    - num_words: Number of words to generate.\n",
    "    - device: \"cuda\" or \"cpu\".\n",
    "\n",
    "    Returns:\n",
    "    - Generated text (string).\n",
    "    \"\"\"\n",
    "    model.eval()  # ✅ Set model to evaluation mode\n",
    "    model.to(device)\n",
    "\n",
    "    # 👇 Tokenize the input prompt\n",
    "    input_ids = torch.tensor(tokenizer.encode(prompt), dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    for _ in range(num_words):\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids)  # 🔍 Forward pass\n",
    "            next_token_logits = logits[:, -1, :]  # Take last token's output\n",
    "\n",
    "            # 🎲 Sample the next word (greedy or probabilistic)\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(0)  # Greedy decoding\n",
    "\n",
    "            # Append to input sequence\n",
    "            input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "\n",
    "    # 🔄 Convert token IDs back to text\n",
    "    generated_text = tokenizer.decode(input_ids.squeeze().tolist())\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1000217e-d3aa-4e3b-b9ce-015d6c6c0f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La ciudad de la República de la Universidad de la Universidad de Buenos Aires fue el órgano de fútbol de la República de la República de la República de la República de la\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate text\n",
    "prompt_text = \"La ciudad de\"\n",
    "generated_story = generate_text(model, hf_tokenizer, prompt_text, num_words=30)\n",
    "print(generated_story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e5905f-f074-49bd-8d24-fb8cbc7d2dc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
